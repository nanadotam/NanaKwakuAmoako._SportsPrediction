# -*- coding: utf-8 -*-
"""FIFA Assignment 2 - Nana Amoako.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1I53d7yX7lrl3wS6gJomWLblYPDtzKWPL

# Regression: FIFA
## Nana Kwaku Amoako
### Assignment 2

### import modules
"""

import pandas as pd
import matplotlib.pyplot as plt
import os
import numpy as np
import pandas as pd
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor

from google.colab import drive
drive.mount('/content/drive')

"""### Load datasets"""

male_players = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/Colab Datasets/male_players (legacy).csv")

"""### DATA PRE-PROCESSING

Basic info and shape
"""

male_players.info()

male_players.shape

male_players.value_counts()

"""use isnull and a file to check the null values thing

### Check for NAs
"""

dataset = male_players

no_nas = True
for col in dataset.columns:
    if dataset[col].isnull().any():
        print(f"There are nulls in: {col}")
        no_nas = False

if no_nas:
    print("Yup, all good no NAs in any column")

"""### Check for columns with NAs greater than a giving threshold"""

# dataset name goes here
dataset = male_players

# threshold for missing values (30%)
threshold = 0.30

# calculate the threshold count for missing values
threshold_count = int(threshold * len(dataset))

missing_columns = [col for col in dataset.columns if dataset[col].isnull().sum() > threshold_count]
print("Successfully checked for missing columns")
print(f"No. of missing columns: {len(missing_columns)}")

missing_columns

# drop columns with more than threshold
male_players_cln = dataset.drop(columns=missing_columns)

print("Number of remaining columns:", len(male_players_cln.columns))
print("Remaining columns:")
for col in male_players_cln.columns:
    print(col)

"""### Remove them through automation"""

male_players_cln

male_players_cln.describe()

"""### CORRELATION MATRIX

Feature subsets: Selecting the best correlated features
"""

correlation_matrix = male_players_cln.select_dtypes(include=[np.number]).corr()

"""map the correlation based on the overall column to check how the overall rating of players correlates with the other features"""

corr_overall = correlation_matrix['overall'].sort_values(ascending=False)

corr_overall

selected_cols = [
                  'overall', 'passing', 'dribbling', 'potential', 'attacking_short_passing', 'shooting',
                  'skill_long_passing', 'skill_ball_control', 'attacking_crossing',
                  'skill_fk_accuracy', 'mentality_vision', 'mentality_aggression',
                  'mentality_positioning', 'mentality_penalties', 'movement_reactions',
                  'potential', 'wage_eur', 'value_eur', 'international_reputation', 'physic',
                  'age', "goalkeeping_diving", "goalkeeping_handling", "goalkeeping_kicking",
                  "goalkeeping_positioning", "goalkeeping_reflexes"
]

df = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/Colab Datasets/male_players (legacy).csv", usecols = selected_cols)

df

df.info()

"""fill NAs"""

# fill Nas with median of columns
df.fillna(df.median(), inplace=True)

df.shape

df

no_nas = True
for col in df.columns:
    if df[col].isnull().any():
        print(f"There are nulls in: {col}")
        no_nas = False

if no_nas:
    print("Yup, all good no NAs in any column")

"""# FEATURE ENGINEERING

Combining similar features
"""

mentality = ["mentality_aggression", "mentality_positioning",
    "mentality_vision", "mentality_penalties"]

df['mentality'] = df[mentality].mean(axis=1)

df.drop(columns=mentality, inplace=True)

df.info()

technical_skills = ["skill_fk_accuracy",
    "skill_long_passing", "skill_ball_control"]

df['technical_skills'] = df[technical_skills].mean(axis=1)

df.drop(columns=technical_skills, inplace=True)

df.info()

goal_keeping = ["goalkeeping_diving", "goalkeeping_handling", "goalkeeping_kicking",
    "goalkeeping_positioning", "goalkeeping_reflexes"]

df['goal_keeping'] = df[goal_keeping].mean(axis=1)

df.drop(columns=goal_keeping, inplace=True)

df.info()

"""### Columns to be used for training (Potential):
Physical Attributes: height_cm, weight_kg

Skill Attributes: pace, shooting, passing, dribbling, defending, physic

Attacking Skills: attacking_crossing, attacking_finishing, attacking_heading_accuracy, attacking_short_passing, attacking_volleys

Skill Moves: skill_dribbling, skill_curve, skill_fk_accuracy, skill_long_passing, skill_ball_control

Movement: movement_acceleration, movement_sprint_speed, movement_agility, movement_reactions, movement_balance

Power: power_shot_power, power_jumping, power_stamina, power_strength, power_long_shots

Mentality: mentality_aggression, mentality_interceptions, mentality_positioning, mentality_vision, mentality_penalties, mentality_composure

Defending: defending_marking_awareness, defending_standing_tackle, defending_sliding_tackle

Goalkeeping: goalkeeping_diving, goalkeeping_handling, goalkeeping_kicking, goalkeeping_positioning, goalkeeping_reflexes

Personal Info: age, preferred_foot, weak_foot, skill_moves, international_reputation, work_rate, body_type

### Imputing
"""

# Imputing
df = df.astype(int)
df.info()

"""## TRAINING MODELS"""

# Target variable (y) = 'overall' (player's overall rating)
y = df['overall']
X = df.drop('overall', axis=1)  # all variables excluding 'overall'

sc = StandardScaler()

scaled = sc.fit_transform(X)

X = pd.DataFrame(scaled, columns=X.columns)

X.info()

X.columns.to_list()

# splitting data into training and testing
# X - training
# y - testing

Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

Xtrain.shape

from sklearn.linear_model import LinearRegression
  from sklearn.tree import DecisionTreeRegressor
  from sklearn.ensemble import GradientBoostingRegressor
  from sklearn.svm import SVR
  from sklearn.neighbors import KNeighborsRegressor
  from sklearn.ensemble import RandomForestRegressor
  from sklearn.ensemble import AdaBoostRegressor
  from sklearn.ensemble import VotingRegressor
  from xgboost import XGBRegressor
  from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

"""Function to automate training and evaluation"""

def train_and_evaluate(model_name, model, Xtrain, Ytrain, Xtest, Ytest):
    model.fit(Xtrain, Ytrain)
    y_pred = model.predict(Xtest)
    mae = mean_absolute_error(Ytest, y_pred)
    mse = mean_squared_error(Ytest, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(Ytest, y_pred)
    print(f"{model_name}:")
    print(f"  Mean Absolute Error: {mae}")
    print(f"  Mean Squared Error: {mse}")
    print(f"  Root Mean Squared Error: {rmse}")
    print(f"  R2 Score: {r2}")
    print()

# Initialize and evaluate Linear Regression
linear_regression = LinearRegression()
train_and_evaluate("Linear Regression", linear_regression, Xtrain, Ytrain, Xtest, Ytest)

# Initialize and evaluate Decision Tree Regressor
decision_tree = DecisionTreeRegressor()
train_and_evaluate("Decision Tree Regressor", decision_tree, Xtrain, Ytrain, Xtest, Ytest)

# Initialize and evaluate AdaBoost Regressor
ada_boost = AdaBoostRegressor()
train_and_evaluate("AdaBoost Regressor", ada_boost, Xtrain, Ytrain, Xtest, Ytest)

# Initialize and evaluate XGB Regressor
xgb_regressor = XGBRegressor()
train_and_evaluate("XGB Regressor", xgb_regressor, Xtrain, Ytrain, Xtest, Ytest)

# Initialize and evaluate Random Forest Regressor
random_forest = RandomForestRegressor()
train_and_evaluate("Random Forest Regressor", random_forest, Xtrain, Ytrain, Xtest, Ytest)

"""# ENSEMBLE LEARNING"""

# Initialize and evaluate Voting Regressor
models = {
    "Random Forest Regressor": RandomForestRegressor(),
    "XGB Regressor": XGBRegressor()
}

voting_regressor = VotingRegressor(estimators=list(models.items()))
train_and_evaluate("Voting Regressor", voting_regressor, Xtrain, Ytrain, Xtest, Ytest)

"""# CROSS-VALIDATION USING GRID SEARCH"""

import pickle as pkl
import joblib
from sklearn.model_selection import GridSearchCV

# hyperparameters for Random Forest
rf_params = {
    'n_estimators': [50, 100],
    'max_depth': [None, 10],
    'min_samples_split': [2, 5]
}

# hyperparameters for XGBoost
xgb_params = {
    'n_estimators': [50, 100],
    'max_depth': [3, 5],
    'learning_rate': [0.1, 0.01,]
}

"""### Initialize the models"""

models = {
    'Random Forest': (RandomForestRegressor(), rf_params),
    'XGB Regressor': (XGBRegressor(), xgb_params),
}

"""Perform grid search for each model"""

'''
loops through the `models` dictionary and passes each model in the grid search
to find the best parameters for each model using negative mean absolute error as
the scoring metric.
'''

best_models = {}

for model_name, (model, params) in models.items():
    grid_search = GridSearchCV(estimator=model, param_grid=params,
                               cv=3, scoring='neg_mean_squared_error',
                               verbose=2, n_jobs=-1)

    grid_search.fit(Xtrain, Ytrain)
    best_params = grid_search.best_params_
    best_models[model_name] = grid_search.best_estimator_
    print(f"Best parameters for {model_name}: {best_params}")

    # Save the best model immediately after finding the best parameters
    joblib.dump(grid_search.best_estimator_, f'/content/drive/MyDrive/Colab Notebooks/best_{model_name.lower()}_model.pkl')
    joblib.dump(grid_search.best_estimator_, f'/content/drive/MyDrive/Colab Notebooks/best_{model_name.lower()}_model.joblib')

    print(f"Model saved")

'''
Code automation to alert me with sound when a long
process is finsihed executing (e.g. model training)
'''

# import os
from IPython.display import Audio, display

# URL to sound
sound_url = 'https://www.soundjay.com/misc/sounds/bell-ringing-01c.mp3'

def play_sound():
    display(Audio(url=sound_url, autoplay=True))

play_sound()
print("Done Cooking! Food is ready!")

"""## ENSEMBLE MODEL

Training with Ensemble RandomForest and XGB
"""

ensemble_model = VotingRegressor(estimators=[('rf', RandomForestRegressor()), ('xgb', XGBRegressor())])
ensemble_model.fit(Xtrain, Ytrain)
# Making predictions using the ensemble model
ensemble_predictions = ensemble_model.predict(Xtest)

# Calculating mean absolute error for the ensemble predictions
mae_ensemble = mean_absolute_error(ensemble_predictions, Ytest)

print("Mean Absolute Error for Ensemble Model:", mae_ensemble)

"""From the above we see that the lower MAE of the ensemble method indicates that it is performing better than the individual models (XGB & RandomForest)

# TEST MODEL WITH NEW DATASET

Pre process new data
"""

new_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Colab Datasets/players_22-1.csv', usecols=selected_cols)

new_df.fillna(0, inplace=True)

dataset = new_df

no_nas = True
for col in dataset.columns:
    if dataset[col].isnull().any():
        print(f"There are nulls in: {col}")
        no_nas = False

if no_nas:
    print(f"Yup, all good no NAs in any column")

"""Feature engineer new data"""

goal_keeping = ["goalkeeping_diving", "goalkeeping_handling", "goalkeeping_kicking",
    "goalkeeping_positioning", "goalkeeping_reflexes"]

new_df['goal_keeping'] = new_df[goal_keeping].mean(axis=1)

new_df.drop(columns=goal_keeping, inplace=True)

new_df.info()

technical_skills = ["skill_fk_accuracy",
    "skill_long_passing", "skill_ball_control"]

new_df['technical_skills'] = new_df[technical_skills].mean(axis=1)

new_df.drop(columns=technical_skills, inplace=True)

new_df.info()

mentality = ["mentality_aggression", "mentality_positioning",
    "mentality_vision", "mentality_penalties"]

new_df['mentality'] = new_df[mentality].mean(axis=1)

new_df.drop(columns=mentality, inplace=True)

new_df.info()

new_df = new_df.astype(int)
new_df.info()

"""TRAINNIG AND EVAL"""

# Target variable (y) = 'overall' (player's overall rating)
y_new = new_df['overall']
X_new = new_df.drop('overall', axis=1)  # all variables excluding 'overall'

sc_new = StandardScaler()
scaled_new = sc_new.fit_transform(X_new)

X_new = pd.DataFrame(scaled_new, columns=X_new.columns)
X_new

X_new.columns.to_list()

# Fix rare class error
class_counts = y_new.value_counts()
threshold = 2

# rare classes
rare_classes = class_counts[class_counts < threshold].index

# combine rare classes into a single class
y_new = y_new.copy().astype(int)
y_new[y_new.isin(rare_classes)] = 92  # 92 is the value for rare classes

# Check the class distribution after combining rare classes
y_new.value_counts()

Xtrain_new, Xtest_new, Ytrain_new, Ytest_new = train_test_split(X_new,
                                                              y_new,
                                                              test_size=0.1,
                                                              random_state=42,
                                                              stratify=y_new)

# Train RandomForest model
rf = RandomForestRegressor()
rf.fit(Xtrain_new, Ytrain_new)
y_pred_rf = rf.predict(Xtest_new)
mae_rf_new = mean_absolute_error(y_pred_rf, Ytest_new)
print(f"Mean Absolute Error for RandomForest: {mae_rf_new}")

# Train XGBoost model
xgb_model = XGBRegressor()
xgb_model.fit(Xtrain_new, Ytrain_new)
y_pred_xgb = xgb_model.predict(Xtest_new)
mae_xgb_new = mean_absolute_error(y_pred_xgb, Ytest_new)
print(f"Mean Absolute Error for XGBoost: {mae_xgb_new}")

# Train Ensemble model
ensemble_model = VotingRegressor(estimators=[('rf', rf), ('xgb', xgb_model)])
ensemble_model.fit(Xtrain_new, Ytrain_new)
ensemble_predictions = ensemble_model.predict(Xtest_new)
mae_ensemble_new = mean_absolute_error(ensemble_predictions, Ytest_new)
print(f"Mean Absolute Error for Ensemble Model: {mae_ensemble_new}")

"""Ensemble prediction good because absolute error is low and closer to 0. model has a good fit to the data and is likely to perform well on unseen data"""

# Save the trained ensemble model to a .pkl file
joblib.dump(ensemble_model, '/content/drive/MyDrive/Colab Notebooks/ensemble_model.pkl')
joblib.dump(ensemble_model, '/content/drive/MyDrive/Colab Notebooks/ensemble_model.joblib')

print(f"Model saved")